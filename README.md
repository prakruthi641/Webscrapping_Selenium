Web Scraping with Selenium

ğŸ“Œ Project Overview

This project is a web scraping script using Selenium to extract data from multiple websites in parallel. The script collects information from specified URLs and stores the extracted data in a structured format, such as a CSV file.

ğŸŒŸ Features:

Uses Selenium WebDriver for automated browsing.

Supports incognito mode for privacy.

Extracts data from multiple websites.

Implements parallel processing with ProcessPoolExecutor for efficiency.

Saves extracted data in a structured CSV format.

ğŸ› ï¸ Tech Stack

Python ğŸ

Selenium WebDriver ğŸŒ

Pandas ğŸ“Š (for data handling)

Concurrent Futures ğŸš€ (for parallel execution)

ğŸ“‚ Project Structure

ğŸ“ Webscraping_Selenium
â”‚â”€â”€ ğŸ“„ scraper.py              # Main web scraping script
â”‚â”€â”€ ğŸ“„ requirements.txt        # Required Python dependencies
â”‚â”€â”€ ğŸ“„ project_list.csv        # Extracted data output
â”‚â”€â”€ ğŸ“„ README.md               # Project Documentation

ğŸš€ Installation & Setup

1ï¸âƒ£ Install Required Dependencies

Ensure you have Python 3.x installed. Then, install the required dependencies:

pip install -r requirements.txt

2ï¸âƒ£ Download & Set Up ChromeDriver

Selenium requires a ChromeDriver to control the Chrome browser. Download the appropriate version from:
ChromeDriver Downloads

Then, set the ChromeDriver path in the script:

chromedriver_path = "C:/chromedriver-win64/chromedriver.exe"

3ï¸âƒ£ Run the Scraper

Execute the script to start web scraping:

python scraper.py

ğŸ–¥ï¸ Code Explanation

1ï¸âƒ£ Creating a WebDriver Instance

def create_webdriver():
    service = Service(chromedriver_path)
    return webdriver.Chrome(service=service, options=driver_option)

Initializes Chrome WebDriver in incognito mode.

Uses Service() to define the ChromeDriver path.

2ï¸âƒ£ Web Scraping Logic

browser.get("https://github.com/collections/machine-learning")
projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']")

Opens the target website.

Extracts all project names using XPath.

3ï¸âƒ£ Parallel Processing for Multiple Websites

with ProcessPoolExecutor(max_workers=4) as executor:
    future_results = {executor.submit(scrape_url, url) for url in urlarray}

Runs multiple web scraping tasks in parallel.

Uses ProcessPoolExecutor to speed up execution.

4ï¸âƒ£ Saving Data to CSV

project_df.to_csv("project_list.csv", index=False)
print("Data saved to project_list.csv")

Converts extracted data into a Pandas DataFrame.

Saves the data into a CSV file.

ğŸ“Š Sample Output (CSV Format)

Project Name    Project URL

TensorFlow   https://github.com/tensorflow

PyTorch   https://github.com/pytorch

ğŸ› ï¸ Troubleshooting

Error: chromedriver not found

Ensure ChromeDriver path is correct.

Check Chrome version compatibility with ChromeDriver.

Selenium Timeout Exception

Increase time.sleep() duration.

Use WebDriverWait to ensure elements load properly.

